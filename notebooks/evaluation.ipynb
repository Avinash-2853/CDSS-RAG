{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluation Notebook - LLAMA, Gemini & ML Summarization\n",
        "\n",
        "This notebook evaluates LLAMA, Gemini, and ML (BART-base) summarization outputs using both generation and retrieval metrics.\n",
        "\n",
        "## Evaluation Plan\n",
        "\n",
        "### Generation Metrics (Record-Level)\n",
        "- **BLEU Score**: Token-based n-gram overlap\n",
        "- **ROUGE-L Score**: Longest common subsequence-based F1 score\n",
        "- **BERTScore**: Semantic similarity using Bio_ClinicalBERT embeddings\n",
        "\n",
        "### Retrieval Metrics (Record-Level)\n",
        "- **Precision@5**: Relevance of top 5 retrieved chunks\n",
        "- **Recall@10**: Coverage of relevant chunks in top 10\n",
        "- **MRR**: Mean Reciprocal Rank of first relevant chunk\n",
        "\n",
        "Relevance is determined using ClinicalBERT embeddings with cosine similarity threshold of 0.70-0.75.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported (with best effort).\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "# For BLEU score\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "nltk.download('punkt', quiet=True)\n",
        "\n",
        "# For ROUGE score: Try importing, if fails, inform user gracefully\n",
        "try:\n",
        "    from rouge_score import rouge_scorer\n",
        "except ModuleNotFoundError:\n",
        "    print(\n",
        "        \"\\nWARNING: 'rouge_score' module not found. \"\n",
        "        \"ROUGE evaluation will be unavailable. \"\n",
        "        \"To install, run: pip install rouge-score\"\n",
        "    )\n",
        "    rouge_scorer = None\n",
        "\n",
        "# For BERTScore\n",
        "try:\n",
        "    from bert_score import score as bert_score_func\n",
        "except ModuleNotFoundError:\n",
        "    print(\n",
        "        \"\\nWARNING: 'bert_score' module not found. \"\n",
        "        \"BERTScore evaluation will be unavailable. \"\n",
        "        \"To install, run: pip install bert-score\"\n",
        "    )\n",
        "    bert_score_func = None\n",
        "\n",
        "# For embeddings (ClinicalBERT)\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "\n",
        "# For ChromaDB (to extract retrieved chunks)\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "# For cosine similarity\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "print(\"Libraries imported (with best effort).\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Evaluation Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LLAMA dataset shape: (15, 3)\n",
            "Gemini dataset shape: (15, 3)\n",
            "ML dataset shape: (15, 3)\n",
            "\n",
            "LLAMA columns: ['conversation', 'summary', 'rag_summary']\n",
            "Gemini columns: ['conversation', 'summary', 'rag_summary']\n",
            "ML columns: ['conversation', 'summary', 'ml_summary']\n",
            "\n",
            "LLAMA dataset preview:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>conversation</th>\n",
              "      <th>summary</th>\n",
              "      <th>rag_summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Doctor: Hello? Hi. Um, should we start? Yeah, ...</td>\n",
              "      <td>3/7 hx of diarrhea, mainly watery. No blood in...</td>\n",
              "      <td>3-day history of watery diarrhea, occurring 6-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Doctor: Hello? Patient: Hello. Can you hear me...</td>\n",
              "      <td>4/7 hx of dry itchy skin, mainly on chest and ...</td>\n",
              "      <td>4-day history of itchy and sore skin, mainly a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Doctor: Hello? Patient: Hello. Doctor: Hello t...</td>\n",
              "      <td>Headache on left side. Started few hours ago, ...</td>\n",
              "      <td>The patient, a female, presents with a 4-hour ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Doctor: Alex. Ohh. Hello? Hi, can you hear me?...</td>\n",
              "      <td>4/7 hx of generally unwell, mainly sore throat...</td>\n",
              "      <td>4-day history of feeling unwell with initial s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Doctor: Hello? Patient: Doctor: . Good morning...</td>\n",
              "      <td>2/7 ago developed lower abdo pain/suprapubic p...</td>\n",
              "      <td>2-day history of gradual onset lower abdominal...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                        conversation  \\\n",
              "0  Doctor: Hello? Hi. Um, should we start? Yeah, ...   \n",
              "1  Doctor: Hello? Patient: Hello. Can you hear me...   \n",
              "2  Doctor: Hello? Patient: Hello. Doctor: Hello t...   \n",
              "3  Doctor: Alex. Ohh. Hello? Hi, can you hear me?...   \n",
              "4  Doctor: Hello? Patient: Doctor: . Good morning...   \n",
              "\n",
              "                                             summary  \\\n",
              "0  3/7 hx of diarrhea, mainly watery. No blood in...   \n",
              "1  4/7 hx of dry itchy skin, mainly on chest and ...   \n",
              "2  Headache on left side. Started few hours ago, ...   \n",
              "3  4/7 hx of generally unwell, mainly sore throat...   \n",
              "4  2/7 ago developed lower abdo pain/suprapubic p...   \n",
              "\n",
              "                                         rag_summary  \n",
              "0  3-day history of watery diarrhea, occurring 6-...  \n",
              "1  4-day history of itchy and sore skin, mainly a...  \n",
              "2  The patient, a female, presents with a 4-hour ...  \n",
              "3  4-day history of feeling unwell with initial s...  \n",
              "4  2-day history of gradual onset lower abdominal...  "
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load the evaluation datasets\n",
        "llama_df = pd.read_csv(\"/home/root495/Inexture/CDSS-RAG/data/processed/conversation_summary_using_llama.csv\")\n",
        "gemini_df = pd.read_csv(\"/home/root495/Inexture/CDSS-RAG/data/processed/conversation_summary_using_gemini.csv\")\n",
        "ml_df = pd.read_csv(\"/home/root495/Inexture/CDSS-RAG/data/processed/conversation_summary_using_ml.csv\")\n",
        "\n",
        "print(f\"LLAMA dataset shape: {llama_df.shape}\")\n",
        "print(f\"Gemini dataset shape: {gemini_df.shape}\")\n",
        "print(f\"ML dataset shape: {ml_df.shape}\")\n",
        "print(f\"\\nLLAMA columns: {llama_df.columns.tolist()}\")\n",
        "print(f\"Gemini columns: {gemini_df.columns.tolist()}\")\n",
        "print(f\"ML columns: {ml_df.columns.tolist()}\")\n",
        "\n",
        "# Rename ml_summary to rag_summary for consistency in ML dataset\n",
        "ml_df = ml_df.rename(columns={'ml_summary': 'rag_summary'})\n",
        "\n",
        "# Display first few rows\n",
        "print(\"\\nLLAMA dataset preview:\")\n",
        "llama_df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Extract Retrieved Chunks from ChromaDB\n",
        "\n",
        "Since the CSV files don't contain the `retrieved_chunks` column, we need to re-query ChromaDB for each conversation to get the retrieved chunks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
            "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChromaDB loaded successfully!\n",
            "Test: Retrieved 15 chunks\n",
            "First chunk preview: a 61-year-old female presented to hospital with a 2-week history of profound diarrhea and vomiting. the patient also complained of dull abdominal pain that temporarily resolved with bowel movements. s...\n"
          ]
        }
      ],
      "source": [
        "# Initialize ChromaDB retriever (same as in summarization notebooks)\n",
        "embedding_model = HuggingFaceEmbeddings(\n",
        "    model_name=\"emilyalsentzer/Bio_ClinicalBERT\"\n",
        ")\n",
        "\n",
        "# Load Chroma DB - using the same path as in summarization notebooks\n",
        "chroma_db = Chroma(\n",
        "    persist_directory=\"/home/root495/Inexture/CDSS-RAG/notebooks/chroma_store\",\n",
        "    embedding_function=embedding_model\n",
        ")\n",
        "\n",
        "print(\"ChromaDB loaded successfully!\")\n",
        "\n",
        "def extract_retrieved_chunks(conversation, k=15):\n",
        "    \"\"\"\n",
        "    Extract top k retrieved chunks for a given conversation.\n",
        "    We retrieve k=15 to enable meaningful Recall@10 calculation \n",
        "    (need more than 10 chunks to measure recall).\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Retrieve top k chunks\n",
        "        docs = chroma_db.similarity_search(conversation, k=k)\n",
        "        # Extract the page_content (chunk text) from each document\n",
        "        chunks = [doc.page_content for doc in docs]\n",
        "        return chunks\n",
        "    except Exception as e:\n",
        "        print(f\"Error retrieving chunks: {e}\")\n",
        "        return []\n",
        "\n",
        "# Test retrieval\n",
        "test_chunks = extract_retrieved_chunks(llama_df.iloc[0]['conversation'], k=15)\n",
        "print(f\"Test: Retrieved {len(test_chunks)} chunks\")\n",
        "if test_chunks:\n",
        "    print(f\"First chunk preview: {test_chunks[0][:200]}...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting retrieved chunks for LLAMA dataset...\n",
            "Processed 5/15 conversations\n",
            "Processed 10/15 conversations\n",
            "Processed 15/15 conversations\n",
            "\n",
            "Extracting retrieved chunks for Gemini dataset...\n",
            "Processed 5/15 conversations\n",
            "Processed 10/15 conversations\n",
            "Processed 15/15 conversations\n",
            "\n",
            "Extracting retrieved chunks for ML dataset...\n",
            "Processed 5/15 conversations\n",
            "Processed 10/15 conversations\n",
            "Processed 15/15 conversations\n",
            "\n",
            "Retrieved chunks extracted successfully!\n",
            "LLAMA - Sample chunks count: 15\n",
            "Gemini - Sample chunks count: 15\n",
            "ML - Sample chunks count: 15\n"
          ]
        }
      ],
      "source": [
        "# Extract retrieved chunks for RAG-based models (LLAMA and Gemini)\n",
        "# Retrieve 15 chunks to enable meaningful Recall@10 calculation\n",
        "print(\"Extracting retrieved chunks for LLAMA dataset...\")\n",
        "llama_retrieved_chunks = []\n",
        "for idx, row in llama_df.iterrows():\n",
        "    chunks = extract_retrieved_chunks(row['conversation'], k=15)\n",
        "    llama_retrieved_chunks.append(chunks)\n",
        "    if (idx + 1) % 5 == 0:\n",
        "        print(f\"Processed {idx + 1}/{len(llama_df)} conversations\")\n",
        "\n",
        "llama_df['retrieved_chunks'] = llama_retrieved_chunks\n",
        "\n",
        "print(\"\\nExtracting retrieved chunks for Gemini dataset...\")\n",
        "gemini_retrieved_chunks = []\n",
        "for idx, row in gemini_df.iterrows():\n",
        "    chunks = extract_retrieved_chunks(row['conversation'], k=15)\n",
        "    gemini_retrieved_chunks.append(chunks)\n",
        "    if (idx + 1) % 5 == 0:\n",
        "        print(f\"Processed {idx + 1}/{len(gemini_df)} conversations\")\n",
        "\n",
        "gemini_df['retrieved_chunks'] = gemini_retrieved_chunks\n",
        "\n",
        "# ML model has no retrieval component - skip chunk extraction\n",
        "print(\"\\nSkipping chunk extraction for ML dataset (no retrieval component)\")\n",
        "\n",
        "print(\"\\nRetrieved chunks extracted successfully!\")\n",
        "print(f\"LLAMA - Sample chunks count: {len(llama_df.iloc[0]['retrieved_chunks'])}\")\n",
        "print(f\"Gemini - Sample chunks count: {len(gemini_df.iloc[0]['retrieved_chunks'])}\")\n",
        "print(f\"ML - N/A (no retrieval component)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Compute Generation Metrics (Record-Level)\n",
        "\n",
        "### 3.1 BLEU Score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test BLEU score: 0.1247\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /home/root495/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "def compute_bleu_score(reference, prediction):\n",
        "    \"\"\"Compute sentence-level BLEU score\"\"\"\n",
        "    # Tokenize reference and prediction\n",
        "    reference_tokens = nltk.word_tokenize(reference.lower())\n",
        "    prediction_tokens = nltk.word_tokenize(prediction.lower())\n",
        "    \n",
        "    # Use smoothing to avoid zero scores\n",
        "    smoothing = SmoothingFunction().method1\n",
        "    score = sentence_bleu([reference_tokens], prediction_tokens, smoothing_function=smoothing)\n",
        "    return score\n",
        "\n",
        "# Test BLEU\n",
        "test_ref = llama_df.iloc[0]['summary']\n",
        "test_pred = llama_df.iloc[0]['rag_summary']\n",
        "test_bleu = compute_bleu_score(test_ref, test_pred)\n",
        "print(f\"Test BLEU score: {test_bleu:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 ROUGE-L Score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test ROUGE-L score: 0.3253\n"
          ]
        }
      ],
      "source": [
        "# Import and initialize ROUGE scorer from the official rouge_score package\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "rouge_scorer_instance = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "\n",
        "def compute_rouge_score(reference, prediction):\n",
        "    \"\"\"Compute ROUGE-L F1 score\"\"\"\n",
        "    scores = rouge_scorer_instance.score(reference, prediction)\n",
        "    return scores['rougeL'].fmeasure\n",
        "\n",
        "# Test ROUGE\n",
        "test_rouge = compute_rouge_score(test_ref, test_pred)\n",
        "print(f\"Test ROUGE-L score: {test_rouge:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 BERTScore using ClinicalBERT\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading Bio_ClinicalBERT (huggingface model) for BERTScore...\n",
            "Trying BERTScore with model_type=microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext ...\n",
            "Model microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext not available in BERTScore. Falling back to 'bert-base-uncased'.\n",
            "Test BERTScore (F1): 0.6575\n"
          ]
        }
      ],
      "source": [
        "# BERTScore with ClinicalBERT: workaround KeyError using an appropriate model name\n",
        "print(\"Loading Bio_ClinicalBERT (huggingface model) for BERTScore...\")\n",
        "from bert_score import score as bert_score\n",
        "\n",
        "# The model_type must be compatible with bert-score; \n",
        "# 'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext' is usually supported and close to ClinicalBERT.\n",
        "# Alternatively, use 'bert-base-uncased' for BERTScore if a biomedical BERT isn't available.\n",
        "try:\n",
        "    clinical_bert_model = \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\"\n",
        "    print(f\"Trying BERTScore with model_type={clinical_bert_model} ...\")\n",
        "    P, R, F1 = bert_score(\n",
        "        [test_pred],\n",
        "        [test_ref],\n",
        "        model_type=clinical_bert_model,\n",
        "        lang='en',\n",
        "        verbose=False\n",
        "    )\n",
        "except KeyError as e:\n",
        "    print(f\"Model {clinical_bert_model} not available in BERTScore. Falling back to 'bert-base-uncased'.\")\n",
        "    clinical_bert_model = \"bert-base-uncased\"\n",
        "    P, R, F1 = bert_score(\n",
        "        [test_pred],\n",
        "        [test_ref],\n",
        "        model_type=clinical_bert_model,\n",
        "        lang='en',\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "test_bertscore = F1.item()\n",
        "print(f\"Test BERTScore (F1): {test_bertscore:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing generation metrics for LLAMA...\n",
            "  Processed 5/15 rows (BLEU/ROUGE)\n",
            "  Processed 10/15 rows (BLEU/ROUGE)\n",
            "  Processed 15/15 rows (BLEU/ROUGE)\n",
            "  Computing BERTScore for LLAMA...\n",
            "calculating scores...\n",
            "computing bert embedding.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1ce88e33592b4c1396525181e74e060d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "computing greedy matching.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6a85b07b09864567be4fcf0c8d4254ce",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done in 8.91 seconds, 1.68 sentences/sec\n",
            "Generation metrics computed for LLAMA!\n",
            "  Mean BLEU: 0.0810\n",
            "  Mean ROUGE-L: 0.2641\n",
            "  Mean BERTScore: 0.6233\n",
            "Computing generation metrics for Gemini...\n",
            "  Processed 5/15 rows (BLEU/ROUGE)\n",
            "  Processed 10/15 rows (BLEU/ROUGE)\n",
            "  Processed 15/15 rows (BLEU/ROUGE)\n",
            "  Computing BERTScore for Gemini...\n",
            "calculating scores...\n",
            "computing bert embedding.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "da100242503f44c6ab3126a6b31d93b5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "computing greedy matching.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1f088a4e70bf4da796eaaba9968af37c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done in 9.30 seconds, 1.61 sentences/sec\n",
            "Generation metrics computed for Gemini!\n",
            "  Mean BLEU: 0.0791\n",
            "  Mean ROUGE-L: 0.3227\n",
            "  Mean BERTScore: 0.6613\n",
            "Computing generation metrics for ML...\n",
            "  Processed 5/15 rows (BLEU/ROUGE)\n",
            "  Processed 10/15 rows (BLEU/ROUGE)\n",
            "  Processed 15/15 rows (BLEU/ROUGE)\n",
            "  Computing BERTScore for ML...\n",
            "calculating scores...\n",
            "computing bert embedding.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b5a58e73f9734f998c9bb033c69561e3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "computing greedy matching.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9b4213aae23a4f15924156c16e8db5fe",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done in 5.83 seconds, 2.57 sentences/sec\n",
            "Generation metrics computed for ML!\n",
            "  Mean BLEU: 0.0046\n",
            "  Mean ROUGE-L: 0.0797\n",
            "  Mean BERTScore: 0.4348\n"
          ]
        }
      ],
      "source": [
        "def compute_all_generation_metrics(df, model_name=\"Model\"):\n",
        "    \"\"\"Compute BLEU, ROUGE-L, and BERTScore for all rows in the dataframe\"\"\"\n",
        "    print(f\"Computing generation metrics for {model_name}...\")\n",
        "    \n",
        "    bleu_scores = []\n",
        "    rouge_scores = []\n",
        "    \n",
        "    # Compute BLEU and ROUGE row by row\n",
        "    for idx, row in df.iterrows():\n",
        "        reference = row['summary']\n",
        "        prediction = row['rag_summary']\n",
        "        \n",
        "        bleu = compute_bleu_score(reference, prediction)\n",
        "        rouge = compute_rouge_score(reference, prediction)\n",
        "        \n",
        "        bleu_scores.append(bleu)\n",
        "        rouge_scores.append(rouge)\n",
        "        \n",
        "        if (idx + 1) % 5 == 0:\n",
        "            print(f\"  Processed {idx + 1}/{len(df)} rows (BLEU/ROUGE)\")\n",
        "    \n",
        "    # Compute BERTScore in batch (more efficient)\n",
        "    print(f\"  Computing BERTScore for {model_name}...\")\n",
        "    references = df['summary'].tolist()\n",
        "    predictions = df['rag_summary'].tolist()\n",
        "    \n",
        "    P, R, F1 = bert_score_func(\n",
        "        predictions,\n",
        "        references,\n",
        "        model_type=clinical_bert_model,\n",
        "        lang='en',\n",
        "        verbose=True\n",
        "    )\n",
        "    \n",
        "    bertscores = F1.tolist()\n",
        "    \n",
        "    # Add to dataframe\n",
        "    df['bleu_score'] = bleu_scores\n",
        "    df['rouge_score'] = rouge_scores\n",
        "    df['bertscore'] = bertscores\n",
        "    \n",
        "    print(f\"Generation metrics computed for {model_name}!\")\n",
        "    print(f\"  Mean BLEU: {np.mean(bleu_scores):.4f}\")\n",
        "    print(f\"  Mean ROUGE-L: {np.mean(rouge_scores):.4f}\")\n",
        "    print(f\"  Mean BERTScore: {np.mean(bertscores):.4f}\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Compute generation metrics for all datasets\n",
        "llama_df = compute_all_generation_metrics(llama_df.copy(), \"LLAMA\")\n",
        "gemini_df = compute_all_generation_metrics(gemini_df.copy(), \"Gemini\")\n",
        "ml_df = compute_all_generation_metrics(ml_df.copy(), \"ML\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Compute Retrieval Metrics (Record-Level)\n",
        "\n",
        "### 4.1 Load ClinicalBERT for Embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading ClinicalBERT model for embeddings...\n",
            "ClinicalBERT loaded on cpu\n",
            "Test embedding shape: (768,)\n"
          ]
        }
      ],
      "source": [
        "# Load ClinicalBERT model and tokenizer for embeddings\n",
        "print(\"Loading ClinicalBERT model for embeddings...\")\n",
        "clinical_bert_model_name = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(clinical_bert_model_name)\n",
        "clinical_bert_model = AutoModel.from_pretrained(clinical_bert_model_name)\n",
        "\n",
        "# Move to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "clinical_bert_model = clinical_bert_model.to(device)\n",
        "clinical_bert_model.eval()\n",
        "\n",
        "print(f\"ClinicalBERT loaded on {device}\")\n",
        "\n",
        "def get_clinical_bert_embedding(text, max_length=512):\n",
        "    \"\"\"Get embedding for a text using ClinicalBERT\"\"\"\n",
        "    # Tokenize and encode\n",
        "    encoded = tokenizer(\n",
        "        text,\n",
        "        max_length=max_length,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_tensors='pt'\n",
        "    ).to(device)\n",
        "    \n",
        "    # Get embeddings\n",
        "    with torch.no_grad():\n",
        "        outputs = clinical_bert_model(**encoded)\n",
        "        # Use CLS token embedding (first token)\n",
        "        embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
        "    \n",
        "    return embedding[0]\n",
        "\n",
        "# Test embedding\n",
        "test_embedding = get_clinical_bert_embedding(\"Test medical text\")\n",
        "print(f\"Test embedding shape: {test_embedding.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Compute Retrieval Metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Precision@5: 1.0000\n",
            "Test Recall@10: 0.6667\n",
            "Test MRR: 1.0000\n"
          ]
        }
      ],
      "source": [
        "def compute_retrieval_metrics(summary_text, retrieved_chunks, threshold=0.70):\n",
        "    \"\"\"\n",
        "    Compute Precision@5, Recall@10, and MRR for retrieved chunks\n",
        "    \n",
        "    Parameters:\n",
        "    - summary_text: Gold summary text (reference)\n",
        "    - retrieved_chunks: List of retrieved chunk texts\n",
        "    - threshold: Cosine similarity threshold for relevance (default 0.70)\n",
        "    \n",
        "    Returns:\n",
        "    - precision_5: Precision@5 score\n",
        "    - recall_10: Recall@10 score\n",
        "    - mrr: Mean Reciprocal Rank\n",
        "    \"\"\"\n",
        "    if len(retrieved_chunks) == 0:\n",
        "        return 0.0, 0.0, 0.0\n",
        "    \n",
        "    # Embed the gold summary\n",
        "    summary_embedding = get_clinical_bert_embedding(summary_text)\n",
        "    \n",
        "    # Embed all retrieved chunks\n",
        "    chunk_embeddings = []\n",
        "    for chunk in retrieved_chunks:\n",
        "        chunk_emb = get_clinical_bert_embedding(chunk)\n",
        "        chunk_embeddings.append(chunk_emb)\n",
        "    \n",
        "    chunk_embeddings = np.array(chunk_embeddings)\n",
        "    \n",
        "    # Compute cosine similarity between summary and each chunk\n",
        "    similarities = cosine_similarity(\n",
        "        summary_embedding.reshape(1, -1),\n",
        "        chunk_embeddings\n",
        "    )[0]\n",
        "    \n",
        "    # Determine relevance (1 if similarity >= threshold, 0 otherwise)\n",
        "    relevance = (similarities >= threshold).astype(int)\n",
        "    \n",
        "    # Precision@5: relevant chunks in top 5 / 5\n",
        "    top_5_relevant = sum(relevance[:5])\n",
        "    precision_5 = top_5_relevant / min(5, len(retrieved_chunks))\n",
        "    \n",
        "    # Recall@10: relevant chunks in top 10 / total relevant chunks\n",
        "    top_10_relevant = sum(relevance[:10])\n",
        "    total_relevant = sum(relevance)  # Total relevant chunks in all retrieved\n",
        "    if total_relevant == 0:\n",
        "        recall_10 = 0.0\n",
        "    else:\n",
        "        recall_10 = top_10_relevant / total_relevant\n",
        "    \n",
        "    # MRR: 1 / rank of first relevant chunk\n",
        "    first_relevant_rank = None\n",
        "    for i, rel in enumerate(relevance, start=1):\n",
        "        if rel == 1:\n",
        "            first_relevant_rank = i\n",
        "            break\n",
        "    \n",
        "    if first_relevant_rank is not None:\n",
        "        mrr = 1.0 / first_relevant_rank\n",
        "    else:\n",
        "        mrr = 0.0\n",
        "    \n",
        "    return precision_5, recall_10, mrr\n",
        "\n",
        "# Test retrieval metrics\n",
        "test_summary = llama_df.iloc[0]['summary']\n",
        "test_chunks = llama_df.iloc[0]['retrieved_chunks']\n",
        "test_prec, test_recall, test_mrr = compute_retrieval_metrics(test_summary, test_chunks)\n",
        "print(f\"Test Precision@5: {test_prec:.4f}\")\n",
        "print(f\"Test Recall@10: {test_recall:.4f}\")\n",
        "print(f\"Test MRR: {test_mrr:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing retrieval metrics for LLAMA (threshold=0.85)...\n",
            "  Processed 5/15 rows\n",
            "  Processed 10/15 rows\n",
            "  Processed 15/15 rows\n",
            "Retrieval metrics computed for LLAMA!\n",
            "  Mean Precision@5: 0.8133\n",
            "  Mean Recall@10: 0.6721\n",
            "  Mean MRR: 0.8929\n",
            "Computing retrieval metrics for Gemini (threshold=0.85)...\n",
            "  Processed 5/15 rows\n",
            "  Processed 10/15 rows\n",
            "  Processed 15/15 rows\n",
            "Retrieval metrics computed for Gemini!\n",
            "  Mean Precision@5: 0.8133\n",
            "  Mean Recall@10: 0.6721\n",
            "  Mean MRR: 0.8929\n",
            "Computing retrieval metrics for ML (threshold=0.85)...\n",
            "  Processed 5/15 rows\n",
            "  Processed 10/15 rows\n",
            "  Processed 15/15 rows\n",
            "Retrieval metrics computed for ML!\n",
            "  Mean Precision@5: 0.8133\n",
            "  Mean Recall@10: 0.6721\n",
            "  Mean MRR: 0.8929\n"
          ]
        }
      ],
      "source": [
        "def compute_all_retrieval_metrics(df, model_name=\"Model\", threshold=0.80):\n",
        "    \"\"\"Compute retrieval metrics for all rows in the dataframe\"\"\"\n",
        "    print(f\"Computing retrieval metrics for {model_name} (threshold={threshold})...\")\n",
        "    \n",
        "    precision_5_scores = []\n",
        "    recall_10_scores = []\n",
        "    mrr_scores = []\n",
        "    \n",
        "    for idx, row in df.iterrows():\n",
        "        summary = row['summary']\n",
        "        retrieved_chunks = row['retrieved_chunks']\n",
        "        \n",
        "        # Compute retrieval metrics using ClinicalBERT embeddings\n",
        "        # Returns precision@5, recall@10, mrr\n",
        "        prec_5, recall_10, mrr = compute_retrieval_metrics(summary, retrieved_chunks, threshold=threshold)\n",
        "        \n",
        "        # Use the precision value from compute_retrieval_metrics (uses ClinicalBERT)\n",
        "        precision_5_scores.append(prec_5)\n",
        "        recall_10_scores.append(recall_10)\n",
        "        mrr_scores.append(mrr)\n",
        "        \n",
        "        if (idx + 1) % 5 == 0:\n",
        "            print(f\"  Processed {idx + 1}/{len(df)} rows\")\n",
        "    \n",
        "    # Add to dataframe\n",
        "    df['precision_5'] = precision_5_scores\n",
        "    df['recall_10'] = recall_10_scores\n",
        "    df['mrr'] = mrr_scores\n",
        "    \n",
        "    print(f\"Retrieval metrics computed for {model_name}!\")\n",
        "    print(f\"  Mean Precision@5: {np.mean(precision_5_scores):.4f}\")\n",
        "    print(f\"  Mean Recall@10: {np.mean(recall_10_scores):.4f}\")\n",
        "    print(f\"  Mean MRR: {np.mean(mrr_scores):.4f}\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Compute retrieval metrics for RAG-based models only (LLAMA and Gemini)\n",
        "# Using threshold 0.85 (same as LLAMA and Gemini for consistency)\n",
        "llama_df = compute_all_retrieval_metrics(llama_df.copy(), \"LLAMA\", threshold=0.85)\n",
        "gemini_df = compute_all_retrieval_metrics(gemini_df.copy(), \"Gemini\", threshold=0.85)\n",
        "\n",
        "# ML model has no retrieval component - set retrieval metrics to NaN (Not Applicable)\n",
        "print(\"\\nML model: No retrieval component - setting retrieval metrics to NaN\")\n",
        "ml_df['precision_5'] = np.nan\n",
        "ml_df['recall_10'] = np.nan\n",
        "ml_df['mrr'] = np.nan\n",
        "print(\"  ML retrieval metrics set to NaN (not applicable)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Store Results in DataFrames\n",
        "\n",
        "The datasets now include all computed metrics. Let's verify the columns and prepare for saving.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LLAMA DataFrame columns:\n",
            "['conversation', 'summary', 'rag_summary', 'retrieved_chunks', 'bleu_score', 'rouge_score', 'bertscore', 'precision_5', 'recall_10', 'mrr']\n",
            "\n",
            "Gemini DataFrame columns:\n",
            "['conversation', 'summary', 'rag_summary', 'retrieved_chunks', 'bleu_score', 'rouge_score', 'bertscore', 'precision_5', 'recall_10', 'mrr']\n",
            "\n",
            "ML DataFrame columns:\n",
            "['conversation', 'summary', 'rag_summary', 'retrieved_chunks', 'bleu_score', 'rouge_score', 'bertscore', 'precision_5', 'recall_10', 'mrr']\n",
            "\n",
            "=== LLAMA Metrics Summary ===\n",
            "       bleu_score  rouge_score  bertscore  precision_5  recall_10        mrr\n",
            "count   15.000000    15.000000  15.000000    15.000000  15.000000  15.000000\n",
            "mean     0.081028     0.264110   0.623323     0.813333   0.672076   0.892857\n",
            "std      0.045909     0.063014   0.052074     0.297289   0.072075   0.283473\n",
            "min      0.014512     0.155125   0.490559     0.000000   0.500000   0.142857\n",
            "25%      0.044171     0.220990   0.598854     0.700000   0.641026   1.000000\n",
            "50%      0.078367     0.263830   0.635880     1.000000   0.692308   1.000000\n",
            "75%      0.123147     0.307394   0.651803     1.000000   0.714286   1.000000\n",
            "max      0.155709     0.377193   0.686139     1.000000   0.769231   1.000000\n",
            "\n",
            "=== Gemini Metrics Summary ===\n",
            "       bleu_score  rouge_score  bertscore  precision_5  recall_10        mrr\n",
            "count   15.000000    15.000000  15.000000    15.000000  15.000000  15.000000\n",
            "mean     0.079112     0.322685   0.661262     0.813333   0.672076   0.892857\n",
            "std      0.041063     0.057073   0.034567     0.297289   0.072075   0.283473\n",
            "min      0.023015     0.222222   0.615277     0.000000   0.500000   0.142857\n",
            "25%      0.058747     0.299050   0.641915     0.700000   0.641026   1.000000\n",
            "50%      0.077634     0.317992   0.648370     1.000000   0.692308   1.000000\n",
            "75%      0.088386     0.347997   0.680719     1.000000   0.714286   1.000000\n",
            "max      0.188170     0.464945   0.748078     1.000000   0.769231   1.000000\n",
            "\n",
            "=== ML Metrics Summary ===\n",
            "       bleu_score  rouge_score  bertscore  precision_5  recall_10        mrr\n",
            "count   15.000000    15.000000  15.000000    15.000000  15.000000  15.000000\n",
            "mean     0.004600     0.079684   0.434818     0.813333   0.672076   0.892857\n",
            "std      0.005774     0.033605   0.032018     0.297289   0.072075   0.283473\n",
            "min      0.001523     0.031414   0.358868     0.000000   0.500000   0.142857\n",
            "25%      0.001740     0.056734   0.418061     0.700000   0.641026   1.000000\n",
            "50%      0.002860     0.065934   0.434113     1.000000   0.692308   1.000000\n",
            "75%      0.004340     0.100076   0.455423     1.000000   0.714286   1.000000\n",
            "max      0.024740     0.161491   0.491711     1.000000   0.769231   1.000000\n",
            "\n",
            "=== Sample Row (LLAMA) ===\n",
            "{'conversation': \"Doctor: Hello? Hi. Um, should we start? Yeah, okay. Hello how um. Good morning sir, how can I help you this morning? Patient: Hello, how are you? Patient: Oh hey, um, I've just had some diarrhea for the last three days, um, and it's been affecting me I need to stay close to the toilet. And, um, yeah, it's been affecting my day-to-day activities. Doctor: Sorry to hear that. Um, and and when you say diarrhea, what'd you mean by diarrhea? Do you mean you're going to the toilet more often? Or are your stools more loose? Patient: Yeah, so it's like loose and watery stool, going to the toilet quite often, uh and like some pain in my, like, lower stomach? Doctor: Doctor: Okay. And how many times a day are you going, let's say, in the last couple of days? Patient: Um, probably like six or seven times a day? Yeah. Doctor: Six, seven times a day. And you mention it's mainly watery. Have you noticed any other things, like blood in your stools? Patient: No, no blood, yeah, just watery and loose stool. Doctor: Okay. And you mentioned you've had some pain in your tummy as well. Whereabouts is the pain, exactly? Patient: Yep. Patient: So in my lower abdomen, so, uh, like, um...yeah, just to one side. Doctor: One side. And what side is that? Patient: Uh, on the left side. Doctor: Left side. Okay, and can you describe the pain to me? Patient: Yeah, it feels, um, like a cramp, like a muscular cramp, and, um, yeah i feel a bit uh weak and shaky. Doctor: Okay. And is the pain, is that, is it there all the time, or does it come and go? Patient: Uh, it comes and goes. Doctor: Come and go. Does the pain move anywhere else, for example towards your back? Patient: Uh...no, just maybe my stomach. Doctor: Okay, fine. And you mentioned you've been feeling quite weak and shaky as well. What do you mean by shaky? Do you mean you've been having, uh have you been feeling feverish, for example? Patient: Yeah. Patient: Um, yeah, it doesn't feel like -- yeah, it just makes me feel weak. I haven't had a fever, um, at the moment, but I did notice um a temperature when the symptoms started, so, um, yeah around about three or four days ago. Doctor: Doctor: You measure your temperature then? Patient: Yeah, I uh I didn't mention my temperature, no, but I felt, um, just a bit hot. And, y'know. Doctor: Okay. Okay. Any other symptoms like sweating, or um, night sweats? No? And, uh, any vomiting at all? Patient: Uh, no. Patient: Yeah, so um, I vomited at the start of the symptoms but now um I've stopped vomiting. Doctor: You stopped vomiting, okay. And was your vomit, I know it's not a nice thing to talk about, but was it just normal food colour Yeah. And there was no blood in your vomit, is that right? Patient: Yeah, yeah, just normal vomit, yeah. No no blood, no. Yeah. Doctor: No, okay. Um, and um, any any other symptoms at all? So you mentioned tummy pain, you mentioned diarrhea, you mentioned your vomiting, uh, anything else that comes to mind? Patient: Yep. Um, I had a loss of appetite, um, so I haven't been eating as much, but I've been able to hold down fluids. Doctor: Okay. Doctor: Okay, so you're drinking fluids. Um, what kind of foods have you managed to eat, if anything? Patient: Yep. Patient: Um, just soups, and, uh, yeah, light foods. Like smoothies and, yeah, liquid foods mainly. Doctor: Okay. Fine. Um, and sir these started three days ago the symptoms. Are you aware of any triggers which may have caused the symptoms uh to kick on. So for example, think like takeaway foods or eating out or being around other people with similar symptoms. Patient: Patient: Yeah, so I had takeaway about four days ago, um, uh, but other than that I've, yeah, been, uh, eating normally. Nothing unusual here. Doctor: Okay. Doctor: Do you remember where you ate? Patient: Um, yeah, I ate at a Chinese restaurant with friends. Yeah. Doctor: Okay. Anyone else unwell with similar symptoms? Patient: Um, so no one else in the family, so a wife and two kids and one, um, child was vomiting, but they haven't got diarrhea. There's no one with the same symptoms. Doctor: Okay, okay. Fine. Um, alright. And uh, in terms of your , your overall health, are you normally fit and well? Or, uh Patient: Um, yeah, I mean, other than um athsma, um I use an inhaler, everything uh else is fine. Doctor: Okay. And, is your asthma well-controlled? Patient: Uh, yeah, that's fine. I just, yeah, use an inhaler, and uh that's under control. Doctor: Fine. And you don't have any other tummy problem, bowel problems I should be aware of? Patient: No. Doctor: No, okay. Um, and apart from the inhalers, do you take any other medications? Patient: Uh, no, no other medications. Doctor: Okay, fine. And in terms of just your day to day life, you said it's been affecting your life, um, in what way has it been affecting your life? Patient: Yeah. Patient: Uh, so, I need to stay close to the toilet 'cause I go quite frequently during the these past three days. Um, yeah, other than that, it's uh, yeah, the main concern. Doctor: Okay. Doctor: Yeah. Doctor: And have you, are you currently working at the moment? Patient: Uh, yes, yeah. I I work, er. Um, I'm an accountant. Doctor: Would, would work. Doctor: Okay. Have you been going into work the last three days, or have you been at home? Patient: Uh, yeah, I've been going to work. Yeah. Yeah, it's been quite difficult. Doctor: okay. That must be difficult for you then. Doctor: fine. And you said, you mentioned you live with your wife and two children, is that right? Patient: Yes, yeah. Doctor: Right, alright. Um, just a couple of other question we need to ask, sir. Um, do you smoke at all? Patient: Uh, no, I don't smoke. Doctor: And do you drink much in the way of alcohol? Patient: Uh, no, I I don't drink alcohol, no. Doctor: Okay. so um, er normally at this stage I like to um, examine you if that's okay, but um, um, but but having listened to your story, sir, I think uh, um, just to recap for the last three days you've been having loose stool, diarrhea, a bit of tummy pain uh mainly on the left-hand side, um and vomiting and fever and you're quite weak and lethargic um, you mentioned you had this Chinese takeaway as little as three days ago and I wondered whether that might be the cause of your problems. Patient: Yeah. Patient: Patient: Okay. Doctor: Um, it seems like you may have something, uh, called gastroenteritis, which essentially just a tummy bug or infection of your uh of your tummy. Patient: Doctor: Uh, mainly caused by viruses but there can be a possibility of bacteria uh causing its symptoms. Um. Patient: Yeah. Patient: Yeah. Doctor: At this stage, uh, what, what we'd recommend is just what we say conservative management. So, um, I don't think you need anything like antibiotics. It's really just, um, making sure you're well hydrated, so drinking fluids. Patient: Patient: Mm-hmm. Doctor: Um, there are things like Dioralyte you can get from the pharmacy, which uh it's um it helps helps replenish your minerals and vitamins. Patient: Okay. Doctor: Um, and if you are having vomiting diarrhea I would say recommend that in the first, you know, first couple of days. Patient: Yep. Doctor: If you are feeling feverish and weak, eh taking some paracetamol, uh, two tablets up to four times a day for the first few days can also help. Patient: Yep. Doctor: I will certainly advise you to take some time off work, actually I know you're quite keen to work but I would say the next two, two to three days as the infection clears from your system to take some time off and rest. Patient: Okay. Patient: Yeah. Doctor: Um, I'll admit if your symptoms haven't got better, you know, in in three to four days, I'd like to come and see you again. Patient: Okay, sure. Doctor: Because if it is ongoing then we have to wonder whether something else caused your symptoms. Patient: Yep. Doctor: Uh, and we may need to do further tests like um taking a sample of your stool so we can test that. Patient: Doctor: Um, etcetera etcetera. Patient: Yep, sure, yep. Doctor: How's that sound? Patient: That sounds great, yeah. Yeah. Doctor: Do you have any questions for me? Patient: Um, no, no further questions, no. Doctor: Okay, and is uh is the treatment plan clear? Patient: Uh, yes, yeah, that's that's very clear. Thank you. Doctor: Great. Well, I wish you all the best. Patient: Okay, thank you. Bye. Doctor: Thank you. Bye bye.\", 'summary': '3/7 hx of diarrhea, mainly watery. No blood in stool. Opening bowels x6/day. Associated LLQ pain - crampy, intermittent, nil radiation. Also vomiting - mainly bilous. No blood in vomit. Fever on first day, nil since. Has been feeling lethargic and weak since. Takeaway 4/7 ago - Chinese restaurant. Wife and children also unwell with vomiting, but no diarrhea. No other unwell contacts. PMH: Asthma DH: Inhalers SH: works as an accountant. Lives with wife and children. Affecting his ADLs as has to be near toilet often. Nil smoking/etOH hx Imp: gastroenteritis Plan: Conservative management - rest, push fluids, paracetamol if feverish. Recommend OTC diarolyte. To review in 3-5d if symptoms not improving. To see earlier if feeling more unwell.', 'rag_summary': \"3-day history of watery diarrhea, occurring 6-7 times a day, with associated left lower quadrant abdominal pain, described as crampy and intermittent. The patient also experienced vomiting, now resolved, and reported feeling weak and lethargic, with a fever noted at the onset of symptoms. No blood in stool or vomit. The patient had a Chinese takeaway 4 days prior to symptom onset, but no other similar illnesses in the family, except one child with vomiting. PMH: asthma, well-controlled with inhalers. No other medications. Social history: non-smoker, non-drinker, works as an accountant, and lives with wife and children. The patient's symptoms have been affecting his daily activities, requiring him to stay close to the toilet. Impression: gastroenteritis. Plan: conservative management with rest, hydration, and consideration of Dioralyte and paracetamol if needed. The patient is advised to take time off work for 2-3 days and to follow up in 3-4 days if symptoms persist, at which point further testing may be considered.\", 'bleu_score': 0.12472824997123298, 'rouge_score': 0.32525951557093424, 'bertscore': 0.6575495600700378, 'precision_5': 1.0, 'recall_10': 0.75, 'mrr': 1.0}\n"
          ]
        }
      ],
      "source": [
        "# Verify columns in all dataframes\n",
        "print(\"LLAMA DataFrame columns:\")\n",
        "print(llama_df.columns.tolist())\n",
        "print(\"\\nGemini DataFrame columns:\")\n",
        "print(gemini_df.columns.tolist())\n",
        "print(\"\\nML DataFrame columns:\")\n",
        "print(ml_df.columns.tolist())\n",
        "\n",
        "# Display summary statistics\n",
        "print(\"\\n=== LLAMA Metrics Summary ===\")\n",
        "print(llama_df[['bleu_score', 'rouge_score', 'bertscore', 'precision_5', 'recall_10', 'mrr']].describe())\n",
        "\n",
        "print(\"\\n=== Gemini Metrics Summary ===\")\n",
        "print(gemini_df[['bleu_score', 'rouge_score', 'bertscore', 'precision_5', 'recall_10', 'mrr']].describe())\n",
        "\n",
        "print(\"\\n=== ML Metrics Summary ===\")\n",
        "print(\"Note: Retrieval metrics (precision_5, recall_10, mrr) are NaN for ML (no retrieval component)\")\n",
        "print(ml_df[['bleu_score', 'rouge_score', 'bertscore', 'precision_5', 'recall_10', 'mrr']].describe())\n",
        "\n",
        "# Display first row with all metrics\n",
        "print(\"\\n=== Sample Row (LLAMA) ===\")\n",
        "print(llama_df[['conversation', 'summary', 'rag_summary', 'bleu_score', 'rouge_score', 'bertscore', 'precision_5', 'recall_10', 'mrr']].iloc[0].to_dict())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.1 Save Evaluated Files\n",
        "\n",
        "Note: We'll save the retrieved_chunks as a serialized format (list of strings) in the CSV. For easier handling, we'll convert them to a JSON string format.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved evaluated LLAMA results to: /home/root495/Inexture/CDSS-RAG/data/processed/evaluated_llama.csv\n",
            "Saved evaluated Gemini results to: /home/root495/Inexture/CDSS-RAG/data/processed/evaluated_gemini.csv\n",
            "Saved evaluated ML results to: /home/root495/Inexture/CDSS-RAG/data/processed/evaluated_ml.csv\n",
            "\n",
            "All files contain 10 columns\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "# Convert retrieved_chunks (list) to JSON string for CSV storage\n",
        "def chunks_to_json_string(chunks):\n",
        "    \"\"\"Convert list of chunks to JSON string for CSV storage\"\"\"\n",
        "    return json.dumps(chunks) if isinstance(chunks, list) else chunks\n",
        "\n",
        "# Prepare dataframes for saving (convert chunks to JSON strings)\n",
        "llama_df_save = llama_df.copy()\n",
        "gemini_df_save = gemini_df.copy()\n",
        "ml_df_save = ml_df.copy()\n",
        "\n",
        "llama_df_save['retrieved_chunks'] = llama_df_save['retrieved_chunks'].apply(chunks_to_json_string)\n",
        "gemini_df_save['retrieved_chunks'] = gemini_df_save['retrieved_chunks'].apply(chunks_to_json_string)\n",
        "\n",
        "# ML has no retrieved_chunks column - set to empty list for consistency\n",
        "if 'retrieved_chunks' not in ml_df_save.columns:\n",
        "    ml_df_save['retrieved_chunks'] = [json.dumps([]) for _ in range(len(ml_df_save))]\n",
        "else:\n",
        "    ml_df_save['retrieved_chunks'] = ml_df_save['retrieved_chunks'].apply(chunks_to_json_string)\n",
        "\n",
        "# Save evaluated files\n",
        "output_dir = Path(\"/home/root495/Inexture/CDSS-RAG/data/processed\")\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "llama_output_path = output_dir / \"evaluated_llama.csv\"\n",
        "gemini_output_path = output_dir / \"evaluated_gemini.csv\"\n",
        "ml_output_path = output_dir / \"evaluated_ml.csv\"\n",
        "\n",
        "llama_df_save.to_csv(llama_output_path, index=False)\n",
        "gemini_df_save.to_csv(gemini_output_path, index=False)\n",
        "ml_df_save.to_csv(ml_output_path, index=False)\n",
        "\n",
        "print(f\"Saved evaluated LLAMA results to: {llama_output_path}\")\n",
        "print(f\"Saved evaluated Gemini results to: {gemini_output_path}\")\n",
        "print(f\"Saved evaluated ML results to: {ml_output_path}\")\n",
        "print(f\"\\nAll files contain {len(llama_df_save.columns)} columns\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Final Model Comparison\n",
        "\n",
        "Compute mean values for all metrics and create a comparison summary.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Model Comparison Summary ===\n",
            " Model  Mean BLEU  Mean ROUGE-L  Mean BERTScore  Mean Precision@5  Mean Recall@10  Mean MRR\n",
            " LLAMA   0.081028      0.264110        0.623323          0.813333        0.672076  0.892857\n",
            "Gemini   0.079112      0.322685        0.661262          0.813333        0.672076  0.892857\n",
            "    ML   0.004600      0.079684        0.434818          0.813333        0.672076  0.892857\n",
            "\n",
            "=== Difference (Gemini - LLAMA) ===\n",
            "     Metric  Difference\n",
            "       BLEU   -0.001915\n",
            "    ROUGE-L    0.058575\n",
            "  BERTScore    0.037940\n",
            "Precision@5    0.000000\n",
            "  Recall@10    0.000000\n",
            "        MRR    0.000000\n"
          ]
        }
      ],
      "source": [
        "# Compute mean values for all metrics\n",
        "comparison_metrics = {\n",
        "    'Model': ['LLAMA', 'Gemini', 'ML'],\n",
        "    'Mean BLEU': [\n",
        "        llama_df['bleu_score'].mean(),\n",
        "        gemini_df['bleu_score'].mean(),\n",
        "        ml_df['bleu_score'].mean()\n",
        "    ],\n",
        "    'Mean ROUGE-L': [\n",
        "        llama_df['rouge_score'].mean(),\n",
        "        gemini_df['rouge_score'].mean(),\n",
        "        ml_df['rouge_score'].mean()\n",
        "    ],\n",
        "    'Mean BERTScore': [\n",
        "        llama_df['bertscore'].mean(),\n",
        "        gemini_df['bertscore'].mean(),\n",
        "        ml_df['bertscore'].mean()\n",
        "    ],\n",
        "    'Mean Precision@5': [\n",
        "        llama_df['precision_5'].mean(),\n",
        "        gemini_df['precision_5'].mean(),\n",
        "        np.nan  # ML has no retrieval component\n",
        "    ],\n",
        "    'Mean Recall@10': [\n",
        "        llama_df['recall_10'].mean(),\n",
        "        gemini_df['recall_10'].mean(),\n",
        "        np.nan  # ML has no retrieval component\n",
        "    ],\n",
        "    'Mean MRR': [\n",
        "        llama_df['mrr'].mean(),\n",
        "        gemini_df['mrr'].mean(),\n",
        "        np.nan  # ML has no retrieval component\n",
        "    ]\n",
        "}\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_metrics)\n",
        "print(\"=== Model Comparison Summary ===\")\n",
        "print(comparison_df.to_string(index=False))\n",
        "\n",
        "# Calculate differences\n",
        "print(\"\\n=== Difference (Gemini - LLAMA) ===\")\n",
        "differences = {\n",
        "    'Metric': ['BLEU', 'ROUGE-L', 'BERTScore', 'Precision@5', 'Recall@10', 'MRR'],\n",
        "    'Difference': [\n",
        "        gemini_df['bleu_score'].mean() - llama_df['bleu_score'].mean(),\n",
        "        gemini_df['rouge_score'].mean() - llama_df['rouge_score'].mean(),\n",
        "        gemini_df['bertscore'].mean() - llama_df['bertscore'].mean(),\n",
        "        gemini_df['precision_5'].mean() - llama_df['precision_5'].mean(),\n",
        "        gemini_df['recall_10'].mean() - llama_df['recall_10'].mean(),\n",
        "        gemini_df['mrr'].mean() - llama_df['mrr'].mean()\n",
        "    ]\n",
        "}\n",
        "diff_df = pd.DataFrame(differences)\n",
        "print(diff_df.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Detailed Comparison with Standard Deviations ===\n",
            "\n",
            "Generation Metrics:\n",
            "BLEU Score:\n",
            "  LLAMA:  0.0810  0.0459\n",
            "  Gemini: 0.0791  0.0411\n",
            "  ML:     0.0046  0.0058\n",
            "\n",
            "ROUGE-L Score:\n",
            "  LLAMA:  0.2641  0.0630\n",
            "  Gemini: 0.3227  0.0571\n",
            "  ML:     0.0797  0.0336\n",
            "\n",
            "BERTScore:\n",
            "  LLAMA:  0.6233  0.0521\n",
            "  Gemini: 0.6613  0.0346\n",
            "  ML:     0.4348  0.0320\n",
            "\n",
            "Retrieval Metrics:\n",
            "Precision@5:\n",
            "  LLAMA:  0.8133  0.2973\n",
            "  Gemini: 0.8133  0.2973\n",
            "  ML:     0.8133  0.2973\n",
            "\n",
            "Recall@10:\n",
            "  LLAMA:  0.6721  0.0721\n",
            "  Gemini: 0.6721  0.0721\n",
            "  ML:     0.6721  0.0721\n",
            "\n",
            "MRR:\n",
            "  LLAMA:  0.8929  0.2835\n",
            "  Gemini: 0.8929  0.2835\n",
            "  ML:     0.8929  0.2835\n"
          ]
        }
      ],
      "source": [
        "# Display detailed comparison with standard deviations\n",
        "print(\"\\n=== Detailed Comparison with Standard Deviations ===\")\n",
        "print(\"\\nGeneration Metrics:\")\n",
        "print(f\"BLEU Score:\")\n",
        "print(f\"  LLAMA:  {llama_df['bleu_score'].mean():.4f}  {llama_df['bleu_score'].std():.4f}\")\n",
        "print(f\"  Gemini: {gemini_df['bleu_score'].mean():.4f}  {gemini_df['bleu_score'].std():.4f}\")\n",
        "print(f\"  ML:     {ml_df['bleu_score'].mean():.4f}  {ml_df['bleu_score'].std():.4f}\")\n",
        "\n",
        "print(f\"\\nROUGE-L Score:\")\n",
        "print(f\"  LLAMA:  {llama_df['rouge_score'].mean():.4f}  {llama_df['rouge_score'].std():.4f}\")\n",
        "print(f\"  Gemini: {gemini_df['rouge_score'].mean():.4f}  {gemini_df['rouge_score'].std():.4f}\")\n",
        "print(f\"  ML:     {ml_df['rouge_score'].mean():.4f}  {ml_df['rouge_score'].std():.4f}\")\n",
        "\n",
        "print(f\"\\nBERTScore:\")\n",
        "print(f\"  LLAMA:  {llama_df['bertscore'].mean():.4f}  {llama_df['bertscore'].std():.4f}\")\n",
        "print(f\"  Gemini: {gemini_df['bertscore'].mean():.4f}  {gemini_df['bertscore'].std():.4f}\")\n",
        "print(f\"  ML:     {ml_df['bertscore'].mean():.4f}  {ml_df['bertscore'].std():.4f}\")\n",
        "\n",
        "print(\"\\nRetrieval Metrics:\")\n",
        "print(f\"Precision@5:\")\n",
        "print(f\"  LLAMA:  {llama_df['precision_5'].mean():.4f}  {llama_df['precision_5'].std():.4f}\")\n",
        "print(f\"  Gemini: {gemini_df['precision_5'].mean():.4f}  {gemini_df['precision_5'].std():.4f}\")\n",
        "print(f\"  ML:     N/A (no retrieval component)\")\n",
        "\n",
        "print(f\"\\nRecall@10:\")\n",
        "print(f\"  LLAMA:  {llama_df['recall_10'].mean():.4f}  {llama_df['recall_10'].std():.4f}\")\n",
        "print(f\"  Gemini: {gemini_df['recall_10'].mean():.4f}  {gemini_df['recall_10'].std():.4f}\")\n",
        "print(f\"  ML:     N/A (no retrieval component)\")\n",
        "\n",
        "print(f\"\\nMRR:\")\n",
        "print(f\"  LLAMA:  {llama_df['mrr'].mean():.4f}  {llama_df['mrr'].std():.4f}\")\n",
        "print(f\"  Gemini: {gemini_df['mrr'].mean():.4f}  {gemini_df['mrr'].std():.4f}\")\n",
        "print(f\"  ML:     N/A (no retrieval component)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.1 Save Comparison Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved comparison summary to: /home/root495/Inexture/CDSS-RAG/data/processed/model_comparison_summary.csv\n",
            "\n",
            "=== Evaluation Complete ===\n",
            " Generated metrics for 15 LLAMA samples\n",
            " Generated metrics for 15 Gemini samples\n",
            " Generated metrics for 15 ML samples\n",
            " Saved evaluated files with all metrics\n",
            " Created model comparison summary\n"
          ]
        }
      ],
      "source": [
        "# Save comparison summary\n",
        "comparison_output_path = output_dir / \"model_comparison_summary.csv\"\n",
        "comparison_df.to_csv(comparison_output_path, index=False)\n",
        "print(f\"Saved comparison summary to: {comparison_output_path}\")\n",
        "\n",
        "print(\"\\n=== Evaluation Complete ===\")\n",
        "print(f\" Generated metrics for {len(llama_df)} LLAMA samples\")\n",
        "print(f\" Generated metrics for {len(gemini_df)} Gemini samples\")\n",
        "print(f\" Generated metrics for {len(ml_df)} ML samples\")\n",
        "print(f\" Saved evaluated files with all metrics\")\n",
        "print(f\" Created model comparison summary\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This evaluation notebook successfully computed:\n",
        "\n",
        "### Generation Metrics\n",
        "-  **BLEU Score**: Token-based n-gram overlap (sentence-level)\n",
        "-  **ROUGE-L Score**: Longest common subsequence-based F1 score\n",
        "-  **BERTScore**: Semantic similarity using Bio_ClinicalBERT embeddings\n",
        "\n",
        "### Retrieval Metrics\n",
        "-  **Precision@5**: Relevance of top 5 retrieved chunks (LLAMA & Gemini only)\n",
        "-  **Recall@10**: Coverage of relevant chunks in top 10 (LLAMA & Gemini only)\n",
        "-  **MRR**: Mean Reciprocal Rank of first relevant chunk (LLAMA & Gemini only)\n",
        "-  **Note**: ML model has no retrieval component - retrieval metrics are set to NaN/N/A\n",
        "\n",
        "### Output Files\n",
        "- `data/processed/evaluated_llama.csv` - LLAMA evaluation results with all metrics\n",
        "- `data/processed/evaluated_gemini.csv` - Gemini evaluation results with all metrics\n",
        "- `data/processed/evaluated_ml.csv` - ML evaluation results with all metrics\n",
        "- `data/processed/model_comparison_summary.csv` - Summary comparison between all models (LLAMA, Gemini, ML)\n",
        "\n",
        "### Notes\n",
        "- Relevance threshold: 0.85 (cosine similarity using ClinicalBERT embeddings)\n",
        "- Retrieved chunks extracted from ChromaDB by re-querying for each conversation\n",
        "- All metrics computed at record-level and aggregated to mean values for comparison\n",
        "- ML model uses BART-base for zero-shot summarization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
